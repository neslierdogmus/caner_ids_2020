{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "database_directory = '../dataset'\n",
    "file_list = ['Tuesday-WorkingHours.csv',\n",
    "             'Wednesday-WorkingHours.csv',\n",
    "             'Thursday-WorkingHours-Morning-WebAttacks.csv',\n",
    "             'Thursday-WorkingHours-Afternoon-Infilteration.csv',\n",
    "             'Friday-WorkingHours-Morning.csv',\n",
    "             'Friday-WorkingHours-Afternoon-PortScan.csv',\n",
    "             'Friday-WorkingHours-Afternoon-DDos.csv',\n",
    "             'Monday-WorkingHours.csv']\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for f in file_list:\n",
    "    file_name = os.path.join(database_directory, f)\n",
    "    df = df.append(pd.read_csv(file_name, header=0, encoding = 'unicode_escape'), ignore_index=True)\n",
    "    print(f, 'is read.')\n",
    "    \n",
    "print('All files are read.')\n",
    "print('Number of rows:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('',np.nan)\n",
    "df = df.dropna(how='all')\n",
    "print('Empty lines dropped.')\n",
    "print('Number of rows:', len(df))\n",
    "\n",
    "df = df.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "print('Duplicated lines dropped.')\n",
    "print('Number of rows:', len(df))\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df.columns = [\n",
    "    'flow_id',\n",
    "    'source_ip',\n",
    "    'source_port',\n",
    "    'destination_ip',\n",
    "    'destination_port',\n",
    "    'protocol',\n",
    "    'timestamp',\n",
    "    'flow_duration',\n",
    "    'total_fwd_packets',\n",
    "    'total_backward_packets',\n",
    "    'total_length_of_fwd_packets',\n",
    "    'total_length_of_bwd_packets',\n",
    "    'fwd_packet_length_max',\n",
    "    'fwd_packet_length_min',\n",
    "    'fwd_packet_length_mean',\n",
    "    'fwd_packet_length_std',\n",
    "    'bwd_packet_length_max',\n",
    "    'bwd_packet_length_min',\n",
    "    'bwd_packet_length_mean',\n",
    "    'bwd_packet_length_std',\n",
    "    'flow_bytes_s',\n",
    "    'flow_packets_s',\n",
    "    'flow_iat_mean',\n",
    "    'flow_iat_std',\n",
    "    'flow_iat_max',\n",
    "    'flow_iat_min',\n",
    "    'fwd_iat_total',\n",
    "    'fwd_iat_mean',\n",
    "    'fwd_iat_std',\n",
    "    'fwd_iat_max',\n",
    "    'fwd_iat_min',\n",
    "    'bwd_iat_total',\n",
    "    'bwd_iat_mean',\n",
    "    'bwd_iat_std',\n",
    "    'bwd_iat_max',\n",
    "    'bwd_iat_min',\n",
    "    'fwd_psh_flags',\n",
    "    'bwd_psh_flags',\n",
    "    'fwd_urg_flags',\n",
    "    'bwd_urg_flags',\n",
    "    'fwd_header_length',\n",
    "    'bwd_header_length',\n",
    "    'fwd_packets_s',\n",
    "    'bwd_packets_s',\n",
    "    'min_packet_length',\n",
    "    'max_packet_length',\n",
    "    'packet_length_mean',\n",
    "    'packet_length_std',\n",
    "    'packet_length_variance',\n",
    "    'fin_flag_count',\n",
    "    'syn_flag_count',\n",
    "    'rst_flag_count',\n",
    "    'psh_flag_count',\n",
    "    'ack_flag_count',\n",
    "    'urg_flag_count',\n",
    "    'cwe_flag_count',\n",
    "    'ece_flag_count',\n",
    "    'down_up_ratio',\n",
    "    'average_packet_size',\n",
    "    'avg_fwd_segment_size',\n",
    "    'avg_bwd_segment_size',\n",
    "    'fwd_header_length_duplicate',\n",
    "    'fwd_avg_bytes_bulk',\n",
    "    'fwd_avg_packets_bulk',\n",
    "    'fwd_avg_bulk_rate',\n",
    "    'bwd_avg_bytes_bulk',\n",
    "    'bwd_avg_packets_bulk',\n",
    "    'bwd_avg_bulk_rate',\n",
    "    'subflow_fwd_packets',\n",
    "    'subflow_fwd_bytes',\n",
    "    'subflow_bwd_packets',\n",
    "    'subflow_bwd_bytes',\n",
    "    'init_win_bytes_forward',\n",
    "    'init_win_bytes_backward',\n",
    "    'act_data_pkt_fwd',\n",
    "    'min_seg_size_forward',\n",
    "    'active_mean',\n",
    "    'active_std',\n",
    "    'active_max',\n",
    "    'active_min',\n",
    "    'idle_mean',\n",
    "    'idle_std',\n",
    "    'idle_max',\n",
    "    'idle_min',\n",
    "    'label'\n",
    "    ]\n",
    "\n",
    "#removing duplicate fwd_header_length and packet specific data\n",
    "df.drop('fwd_header_length_duplicate', axis=1, inplace=True)\n",
    "df.drop('flow_id', axis=1, inplace=True)\n",
    "df.drop('source_ip', axis=1, inplace=True)\n",
    "df.drop('source_port', axis=1, inplace=True)\n",
    "df.drop('destination_ip', axis=1, inplace=True)\n",
    "df.drop('destination_port', axis=1, inplace=True)\n",
    "df.drop('timestamp', axis=1, inplace=True)\n",
    "print('6 columns (flow_id, source_ip, source_port, destination_ip, destination_port, timestamp) are dropped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.to_numpy()\n",
    "labels = df[:,-1]\n",
    "df = df[:,:-1].astype(float)\n",
    "\n",
    "# find column averages for average_flow_bytes_s and average_flow_packets_s (without nan and inf)\n",
    "data_no_nan = df[~np.isnan(df).any(axis=1)]\n",
    "data_no_inf = data_no_nan[~np.isinf(data_no_nan).any(axis=1)]\n",
    "del data_no_nan\n",
    "average_flow_bytes_s = np.average(data_no_inf[:,14])\n",
    "average_flow_packets_s = np.average(data_no_inf[:,15])\n",
    "del data_no_inf\n",
    "\n",
    "for c in range(df.shape[0]):\n",
    "    if np.isnan(df[c,14]): df[c,14] = 0\n",
    "    if np.isnan(df[c,15]): df[c,15] = 0\n",
    "    if np.isinf(df[c,14]): df[c,14] = average_flow_bytes_s\n",
    "    if np.isinf(df[c,15]): df[c,15] = average_flow_packets_s\n",
    "        \n",
    "print('Nan entries are replaced with zeros and inf entries with the column average.')\n",
    "\n",
    "np.save('../data/features.npy', df)\n",
    "np.save('../data/labels.npy', labels)\n",
    "\n",
    "print('Numpy files saved into the data folder.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_benign = np.load('../data/extra_benign.npy')\n",
    "print('Number of extra benign test samples:', len(extra_benign))\n",
    "numbers = {}\n",
    "for fold in range(1,6):\n",
    "    test_index = np.load('../data/test_index' + str(fold) + '.npy')\n",
    "    mask = np.ones(len(labels), dtype=bool)\n",
    "    mask[test_index,] = False\n",
    "    y_train, y_test_uniform = labels[mask], labels[~mask]\n",
    "    \n",
    "    unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "    unique_test_uniform, counts_test_uniform = np.unique(y_test_uniform, return_counts=True)\n",
    "    for i in range(len(unique_train)):\n",
    "        train_key = '_'.join([unique_train[i], 'tr'])\n",
    "        test_key = '_'.join([unique_train[i], 'te'])\n",
    "        if not train_key in numbers: numbers[train_key] = []\n",
    "        if not test_key in numbers: numbers[test_key] = []\n",
    "        numbers[train_key].append(counts_train[i])\n",
    "        numbers[test_key].append(counts_test_uniform[i])\n",
    "\n",
    "print(pd.DataFrame(numbers).transpose())\n",
    "print(counts_test_uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
